---
concept: 点积的几何意义
aliases: [Dot Product, 内积, Inner Product, 标量积]
source: 3Blue1Brown《线性代数的本质》第8集
week: 1
day: 2
phase: 0
tags: [concept, linear-algebra, W1D2]
---

# 点积的几何意义 (Dot Product)

## 一句话定义

点积是一个向量在另一个向量方向上的**投影长度**乘以另一个向量的长度，本质上是一个 **1×n 矩阵**（即线性变换到一维）。

## 直觉/类比

想象太阳从正上方照下来，向量 $\vec{a}$ 在地面上投下影子：
- 点积 = 影子的长度 × $\vec{b}$ 的长度
- **正值**：两个向量大致同向（夹角 < 90°）
- **零**：两个向量垂直（夹角 = 90°，影子长度为 0）
- **负值**：两个向量大致反向（夹角 > 90°）

软工类比：点积就像**相似度度量**。两个特征向量的点积越大，说明它们越"相似"（方向越接近）。这正是搜索引擎、推荐系统的核心思想。

**3B1B 的核心洞察**：点积不只是"对应元素相乘再求和"的计算公式，它本质上是一种**线性变换**——把 n 维向量映射到 1 维数轴上。

## 数学表达

**代数定义**：
$$\vec{a} \cdot \vec{b} = \sum_{i=1}^{n} a_i b_i = a_1 b_1 + a_2 b_2 + \cdots + a_n b_n$$

**几何定义**：
$$\vec{a} \cdot \vec{b} = \|\vec{a}\| \cdot \|\vec{b}\| \cdot \cos\theta$$

其中 $\theta$ 是两向量的夹角。

**作为线性变换**：
$$\vec{w} \cdot \vec{x} = \begin{pmatrix} w_1 & w_2 & \cdots & w_n \end{pmatrix} \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}$$

即 1×n 矩阵乘以 n×1 向量 → 标量。

**余弦相似度**：
$$\cos\theta = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \cdot \|\vec{b}\|}$$

## Python 代码

```python
import numpy as np

a = np.array([1, 2, 3])
b = np.array([4, 5, 6])

# === 三种等价计算 ===
dot1 = np.dot(a, b)           # 内置函数
dot2 = a @ b                  # 矩阵乘法语法
dot3 = np.sum(a * b)          # 手动：对应相乘再求和
print(f"点积: {dot1} = {dot2} = {dot3}")  # 32

# === 几何意义验证 ===
cos_theta = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
theta = np.arccos(cos_theta)
print(f"夹角: {np.degrees(theta):.1f}°")  # 12.9°

# === 作为 1×n 矩阵变换 ===
w = np.array([[1, 2, 3]])  # 1×3 矩阵
x = np.array([[4], [5], [6]])  # 3×1 向量
print(f"矩阵乘法: {(w @ x)[0,0]}")  # 32，与点积一致

# === 余弦相似度（ML 中最常用的形式）===
from sklearn.metrics.pairwise import cosine_similarity
sim = cosine_similarity([a], [b])
print(f"余弦相似度: {sim[0,0]:.4f}")  # 0.9746

# === 投影 ===
# a 在 b 方向上的投影向量
proj = (np.dot(a, b) / np.dot(b, b)) * b
print(f"a 在 b 上的投影: {proj}")
```

## 在 ML/DL 中的位置

```
点积
├── 神经网络核心：y = w·x + b（每个神经元就是一个点积）
├── 注意力机制：score = Q·K^T（Query 和 Key 的相似度）
├── 余弦相似度：sim = a·b / (|a|·|b|)
│   ├── 词向量相似度（Word2Vec）
│   ├── 搜索引擎的文档匹配
│   └── 推荐系统的用户-物品匹配
├── 损失函数
│   ├── MSE 展开后包含点积
│   └── Hinge Loss 中的 w·x
└── SVM：决策边界 w·x + b = 0
```

**核心认知**：深度学习的绝大多数计算都建立在点积之上。理解点积 = 相似度度量 = 线性变换，就理解了神经网络的基础运算。

## 常见面试问法

1. **点积的几何意义？** → 一个向量在另一个方向上的投影 × 长度，等价于 $\|a\|\|b\|\cos\theta$
2. **为什么 Attention 用点积计算相似度？** → 点积天然衡量方向相似性，计算高效（矩阵乘法）
3. **余弦相似度和点积的区别？** → 余弦相似度归一化了向量长度，只衡量方向；点积同时考虑方向和大小
4. **为什么 Transformer 的 Attention 要除以 $\sqrt{d_k}$？** → 防止高维点积值过大导致 softmax 梯度消失

## 关联概念

- [[非方阵的变换]] — 点积是 1×n 矩阵变换的特例
- [[线性变换]] — 点积是最简单的线性变换（到一维）
- [[行列式]] — 行列式可以用点积和叉积表达
