---
type: concept
created: 2026-02-09
phase: 0
tags: [math,basics,linear-algebra]
aliases: [线性变换, Linear Transformation]
---

# 线性变换 (Linear Transformation)

## 🎯 一句话定义

**线性变换是一种"保持网格线平行且均匀分布"的空间变换**，本质上是对空间的"拉伸、旋转、剪切"等操作，而矩阵就是描述这种变换的"配方"。

---

## 🧠 直觉理解

### 形象比喻

想象你有一张画着网格的透明胶片（像坐标纸），你对这张胶片做以下操作：

| 操作 | 描述 | 是否线性变换？ |
|------|------|----------------|
| 🔄 旋转 | 转动胶片 | ✅ 是 |
| 📏 缩放 | 拉伸或压缩 | ✅ 是 |
| ⬡ 剪切 | 正方形→平行四边形 | ✅ 是 |
| ➡️ 平移 | 移动胶片位置 | ❌ 否（原点动了）|

### 关键特征

变换后：
- 原来的直线 → 还是直线
- 原来平行的线 → 仍然平行
- 原点位置 → **不动**（重要！）

**为什么叫"线性"？两条规则：**
1. **可加性**: `f(x + y) = f(x) + f(y)` —— 先加再变 = 先变再加
2. **齐次性**: `f(k·x) = k·f(x)` —— 拉伸后变 = 变后拉伸

---

## 📐 数学表达

### 矩阵就是变换的"代码"

$$
\begin{bmatrix} a & b \\ c & d \end{bmatrix}
\begin{bmatrix} x \\ y \end{bmatrix}
=
\begin{bmatrix} ax + by \\ cx + dy \end{bmatrix}
$$

**本质理解**:
- **第一列 `[a, c]`** = 基向量 `î`（x轴单位向量）变换后的位置
- **第二列 `[b, d]`** = 基向量 `ĵ`（y轴单位向量）变换后的位置

> **核心洞察**: 要描述任何线性变换，只需要知道基向量去了哪里！

### 常见变换矩阵

| 变换 | 矩阵 | 效果 |
|------|------|------|
| 水平拉伸2倍 | `[[2,0],[0,1]]` | x方向变宽 |
| 旋转90° | `[[0,-1],[1,0]]` | 逆时针旋转 |
| 剪切 | `[[1,1],[0,1]]` | 正方形→平行四边形 |
| 缩放到0 | `[[0,0],[0,0]]` | 所有点压缩到原点 |

---

## 💻 代码实现

```python
import numpy as np

# 定义一个线性变换矩阵（水平拉伸2倍）
A = np.array([[2, 0],
              [0, 1]])

# 原始向量
v = np.array([1, 1])

# 应用变换 = 矩阵乘法
v_transformed = A @ v

print(f"原始向量: {v}")           # [1, 1]
print(f"变换后: {v_transformed}")   # [2, 1]

# 可视化多个点
points = np.array([[1,0], [0,1], [1,1], [2,2]])
transformed = points @ A.T  # 注意：行向量需要右乘矩阵的转置
print(f"\n原始点:\n{points}")
print(f"变换后:\n{transformed}")
```

---

## 🔗 在ML/DL中的位置

### 上游依赖
- [[01-向量]] - 线性变换的作用对象
- [[矩阵]] - 线性变换的表示方式
- [[04-基向量]] - 理解变换的关键

### 下游应用
- [[特征值与特征向量]] - 寻找变换的"主轴"
- [[奇异值分解(SVD)]] - 任何线性变换的分解
- [[../../../02-Deep-Learning/README|神经网络]] - 每一层都是线性变换+激活函数
- [[主成分分析(PCA)]] - 数据的线性变换降维

---

## 💬 面试常见问法

### Q1: 为什么神经网络需要激活函数？

**A**: 如果没有激活函数，多层神经网络 = 一个大的线性变换矩阵（矩阵乘法满足结合律），等价于单层。激活函数引入非线性，让网络能拟合复杂关系。

### Q2: 什么是奇异值分解（SVD）？

**A**: 任何线性变换都可以分解为：旋转 → 缩放 → 旋转。即 `A = UΣVᵀ`，这在数据压缩、推荐系统中应用广泛。

### Q3: 为什么平移不是线性变换？

**A**: 平移会改变原点位置，违反 `f(0) = 0` 的要求。为了解决这个问题，计算机图形学使用齐次坐标（增加一维）来表示平移。

---

## 📚 学习资源

- [x] 3Blue1Brown《线性代数的本质》第3集
- [x] 3Blue1Brown《线性代数的本质》第4集
- [x] [[README|线性代数知识域索引]]

---

## ⚠️ 常见疑惑

### 疑惑1: 矩阵乘法为什么要那样定义？

**解答**: 矩阵乘法的定义正是为了"组合变换"！两个矩阵相乘 = 依次施加两个线性变换。

### 疑惑2: 左乘和右乘有什么区别？

**解答**:
- 列向量（默认）左乘矩阵：`y = Ax`
- 行向量右乘矩阵转置：`y = xAᵀ`
- 几何意义相同，只是表达习惯不同

### 疑惑3: 行列式和线性变换的关系？

**解答**: 行列式的绝对值 = 变换后面积/体积的缩放比例；行列式的符号 = 是否改变了方向（翻转）。

---

# 🔗 反向链接

<!-- 这里会自动显示链接到这个笔记的其他笔记 -->
