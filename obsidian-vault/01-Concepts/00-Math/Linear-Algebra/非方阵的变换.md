---
concept: 非方阵的变换
aliases: [Non-square Matrices, 非方阵, 维度变换]
source: 3Blue1Brown《线性代数的本质》第7集
week: 1
day: 2
phase: 0
tags: [concept, linear-algebra, W1D2]
---

# 非方阵的变换 (Non-square Matrices)

## 一句话定义

非方阵（$m \times n$，$m \neq n$）表示**不同维度之间**的线性变换——将 $n$ 维空间映射到 $m$ 维空间。

## 直觉/类比

- **3×2 矩阵**：把 2D 平面"嵌入"到 3D 空间（升维）
- **2×3 矩阵**：把 3D 空间"投影"到 2D 平面（降维）
- **1×3 矩阵**：把 3D 向量变成一个数（就是点积！）

软工类比：就像数据库的 `SELECT` 和 `JOIN`：
- **降维 (m < n)**：`SELECT col1, col2 FROM table` — 丢掉一些列，保留核心信息
- **升维 (m > n)**：`JOIN` — 给数据增加维度，嵌入到更大的空间

## 数学表达

$m \times n$ 矩阵 $A$：
$$A: \mathbb{R}^n \to \mathbb{R}^m$$

- $n$ = 输入维度（列数）= 定义域维度
- $m$ = 输出维度（行数）= 值域维度

**关键洞察**：矩阵的列数 = 输入空间维度，行数 = 输出空间维度。

例：$2 \times 3$ 矩阵
$$\begin{pmatrix} 2 & 0 & -1 \\ 0 & 1 & 3 \end{pmatrix} \begin{pmatrix} x \\ y \\ z \end{pmatrix} = \begin{pmatrix} 2x - z \\ y + 3z \end{pmatrix}$$

3D → 2D，信息必然有损失。

## Python 代码

```python
import numpy as np

# === 降维：3D → 2D（投影）===
A = np.array([[2, 0, -1],
              [0, 1,  3]])  # 2×3 矩阵

v = np.array([1, 2, 3])  # 3D 向量
result = A @ v
print(f"3D → 2D: {v} → {result}")  # [-1, 11]

# === 升维：2D → 3D（嵌入）===
B = np.array([[1, 0],
              [0, 1],
              [0, 0]])  # 3×2 矩阵

v2 = np.array([3, 4])  # 2D 向量
result2 = B @ v2
print(f"2D → 3D: {v2} → {result2}")  # [3, 4, 0] — 嵌入到 xy 平面

# === 1×n 矩阵 = 点积 ===
w = np.array([[2, 3, 1]])  # 1×3 矩阵
v3 = np.array([1, 2, 3])
print(f"1×3 @ 3×1: {w @ v3}")  # [11] = 2*1 + 3*2 + 1*3
print(f"等价点积: {np.dot([2,3,1], v3)}")  # 11
```

## 在 ML/DL 中的位置

```
非方阵的变换
├── 神经网络全连接层：W @ x + b
│   ├── W 形状 (output_dim × input_dim)
│   └── 每一层都是维度变换！
├── 降维
│   ├── PCA 投影矩阵 (k × n)：n维 → k维
│   └── Embedding 层反向：高维 → 低维
├── 升维
│   ├── Word Embedding (d × V)：词表索引 → 向量
│   └── 神经网络的隐藏层扩展
└── 1×n = 点积 → Attention 的 score 计算
```

**核心认知**：神经网络的每一层 `nn.Linear(in_features, out_features)` 本质上就是一个 $\text{out} \times \text{in}$ 的非方阵变换。整个网络就是一系列维度变换的组合。

## 常见面试问法

1. **nn.Linear(784, 128) 在做什么？** → 一个 128×784 的矩阵变换，把 784 维输入映射到 128 维
2. **为什么 Embedding 层不需要乘法？** → 本质是查表（one-hot × 权重矩阵 = 取出对应行），等价于矩阵乘法但更高效
3. **PCA 降维的矩阵是什么形状？** → $k \times n$，$k$ 是目标维度，$n$ 是原始维度

## 关联概念

- [[线性变换]] — 非方阵是更一般的线性变换
- [[行列式]] — 非方阵没有行列式（只有方阵才有）
- [[点积的几何意义]] — 1×n 矩阵就是点积
